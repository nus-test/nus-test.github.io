<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.2.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Manuel Rigger" />

  
  
  
    
  
  <meta name="description" content="Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies assume implementations of the same algorithm to be interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation discrepancies, their effect on the implementations&#39; performance, as well as their impact on the conclusions of prior studies. The outcome of our differential tests showed significant discrepancies between the tested algorithm implementations. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. Furthermore, the performance among the high-performing PPO implementations was found to differ significantly in nine games. As part of a meticulous manual analysis of the implementations&#39; source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that these implementation discrepancies were sufficient to flip experiment outcomes if left unaccounted for. Therefore, this calls for a shift in how implementations are being used. In addition, we encourage DRL libraries to avoid these discrepancies by either adopting the differential testing methodology proposed in this paper or explicitly documenting code-level inconsistencies." />

  
  <link rel="alternate" hreflang="en-us" href="https://nus-test.github.io/event/240910/" />

  







  




  
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    

    
    
    
      
    
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.299c20016eab2ddaf829d17ab5a7213c.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_huf39aebee443421be528cbdbb4c78ede7_7416_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_huf39aebee443421be528cbdbb4c78ede7_7416_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://nus-test.github.io/event/240910/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
    <meta property="twitter:site" content="@test_nus" />
    <meta property="twitter:creator" content="@test_nus" />
  
  <meta property="og:site_name" content="Trustworthy Engineering of Software Technologies (TEST) Lab" />
  <meta property="og:url" content="https://nus-test.github.io/event/240910/" />
  <meta property="og:title" content="Are Deep Reinforcement Learning Implementations Really Interchangeable? | Trustworthy Engineering of Software Technologies (TEST) Lab" />
  <meta property="og:description" content="Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies assume implementations of the same algorithm to be interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation discrepancies, their effect on the implementations&#39; performance, as well as their impact on the conclusions of prior studies. The outcome of our differential tests showed significant discrepancies between the tested algorithm implementations. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. Furthermore, the performance among the high-performing PPO implementations was found to differ significantly in nine games. As part of a meticulous manual analysis of the implementations&#39; source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that these implementation discrepancies were sufficient to flip experiment outcomes if left unaccounted for. Therefore, this calls for a shift in how implementations are being used. In addition, we encourage DRL libraries to avoid these discrepancies by either adopting the differential testing methodology proposed in this paper or explicitly documenting code-level inconsistencies." /><meta property="og:image" content="https://nus-test.github.io/media/logo_hu54a81beaedce3437486bda7d4bb96888_11488_300x300_fit_lanczos_3.png" />
    <meta property="twitter:image" content="https://nus-test.github.io/media/logo_hu54a81beaedce3437486bda7d4bb96888_11488_300x300_fit_lanczos_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2017-01-01T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2024-09-10T15:00:00&#43;00:00">
  

  



  
    




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Event",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://nus-test.github.io/event/240910/"
  },
  "name": "Are Deep Reinforcement Learning Implementations Really Interchangeable?",
  
  "location": {
    "@type": "Place",
    "name": "COM3-02-59 - Meeting Rm 20",
    "address": {
      "@type": "PostalAddress",
      "streetAddress": "",
      "addressLocality": "",
      "postalCode": "",
      "addressRegion": "",
      "addressCountry": "Singapore"
    }
    
  },
  
  
  "startDate": "2024-09-10T15:00:00Z",
  
  "endDate": "2024-09-10T16:00:00Z",
  
  
  "performer": {
    "@type": "Person",
    "name": "Rajdeep Singh Hundal"
  },
  
  "description": "Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies assume implementations of the same algorithm to be interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation discrepancies, their effect on the implementations' performance, as well as their impact on the conclusions of prior studies. The outcome of our differential tests showed significant discrepancies between the tested algorithm implementations. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. Furthermore, the performance among the high-performing PPO implementations was found to differ significantly in nine games. As part of a meticulous manual analysis of the implementations' source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that these implementation discrepancies were sufficient to flip experiment outcomes if left unaccounted for. Therefore, this calls for a shift in how implementations are being used. In addition, we encourage DRL libraries to avoid these discrepancies by either adopting the differential testing methodology proposed in this paper or explicitly documenting code-level inconsistencies."
}
</script>

  

  

  





  <title>Are Deep Reinforcement Learning Implementations Really Interchangeable? | Trustworthy Engineering of Software Technologies (TEST) Lab</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="c0805f11f43ac4ddcf1cfe64488dc0e8" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.5c87d484a568cb9870aa36d295c02167.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    











  


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/"><img src="/media/logo_hu54a81beaedce3437486bda7d4bb96888_11488_0x70_resize_lanczos_3.png" alt="Trustworthy Engineering of Software Technologies (TEST) Lab"></a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/"><img src="/media/logo_hu54a81beaedce3437486bda7d4bb96888_11488_0x70_resize_lanczos_3.png" alt="Trustworthy Engineering of Software Technologies (TEST) Lab"></a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/project"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/publication"><span>Publications</span></a>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/bugs"><span>Found Bugs</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/positions"><span>Openings</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/event"><span>Seminars</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <div class="pub">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Are Deep Reinforcement Learning Implementations Really Interchangeable?</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/rajdeep-singh-hundal/">Rajdeep Singh Hundal</a></span>
  </div>
  
  

  

  

  

  
  
  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    
      <h3>Abstract</h3>
      <p class="pub-abstract">Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies assume implementations of the same algorithm to be interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation discrepancies, their effect on the implementations&rsquo; performance, as well as their impact on the conclusions of prior studies. The outcome of our differential tests showed significant discrepancies between the tested algorithm implementations. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. Furthermore, the performance among the high-performing PPO implementations was found to differ significantly in nine games. As part of a meticulous manual analysis of the implementations&rsquo; source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that these implementation discrepancies were sufficient to flip experiment outcomes if left unaccounted for. Therefore, this calls for a shift in how implementations are being used. In addition, we encourage DRL libraries to avoid these discrepancies by either adopting the differential testing methodology proposed in this paper or explicitly documenting code-level inconsistencies.</p>
    

    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Date</div>
          <div class="col-12 col-md-9">
            Sep 10, 2024 3:00 PM &mdash; 4:00 PM
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Event</div>
          <div class="col-12 col-md-9">
            
            Weekly Talk
            
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Location</div>
          <div class="col-12 col-md-9">COM3-02-59 - Meeting Rm 20</div>
          
            <div class="col-md-3"></div>
            <div class="col-12 col-md-9"></div>
          
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style">
      
    </div>

    




<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/weekly-talk/">Weekly Talk</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://nus-test.github.io/event/240910/&amp;text=Are%20Deep%20Reinforcement%20Learning%20Implementations%20Really%20Interchangeable?" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://nus-test.github.io/event/240910/&amp;t=Are%20Deep%20Reinforcement%20Learning%20Implementations%20Really%20Interchangeable?" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Are%20Deep%20Reinforcement%20Learning%20Implementations%20Really%20Interchangeable?&amp;body=https://nus-test.github.io/event/240910/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://nus-test.github.io/event/240910/&amp;title=Are%20Deep%20Reinforcement%20Learning%20Implementations%20Really%20Interchangeable?" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Are%20Deep%20Reinforcement%20Learning%20Implementations%20Really%20Interchangeable?%20https://nus-test.github.io/event/240910/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://nus-test.github.io/event/240910/&amp;title=Are%20Deep%20Reinforcement%20Learning%20Implementations%20Really%20Interchangeable?" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="/author/rajdeep-singh-hundal/"><img class="avatar mr-3 avatar-circle" src="/author/rajdeep-singh-hundal/avatar_hucfe07869aad2aec3387c1def3365b71c_23274_270x270_fill_q75_lanczos_center.jpeg" alt="Rajdeep Singh Hundal"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="/author/rajdeep-singh-hundal/">Rajdeep Singh Hundal</a></h5>
      <h6 class="card-subtitle">Ph.D. Student</h6>
      <p class="card-text">Rajdeep is a Ph.D. student.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:rajdeep@u.nus.edu" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/rajdeepsh" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  


















  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  

  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.d3e42e2fc13a6f2096e119f73470f0b6.js"></script>

    






</body>
</html>
